---
layout: default
title: Main
---
# **Jaehoon Lee**

<img align="right" style="float:center;padding:10px;" width="250" src="/image/IMG_2635.JPG">

Google Brain  
1600 Amphitheatre Parkway  
attn: jaehlee  
Mountain View, CA, 94043  

**E-Mail**: eejaehooon at gmail dot com  
**Curriculum Vitae**: [CV](https://jaehlee.github.io/cv_Jaehoon_Lee.pdf)

I am a Research Scientist at [Google Brain Team](https://research.google.com/teams/brain/) interested in understanding deep neural networks. 

I am greatful to have the opportunity to be a part of [AI Residency](https://ai.google/research/join-us/ai-residency) program. 
Before joining Google in 2017, I mostly worked on theoretical physics. 
I was a postdoctoral researcher in the [Department of Physics & Astronomy](http://www.phas.ubc.ca/) at [University of British Columbia (UBC)](http://www.ubc.ca/) in the String Theory Group. 
Before that, I completed my PhD in [Center for Theoretical Physics (CTP)](http://ctp.lns.mit.edu/) at [MIT](http://web.mit.edu/) working on theoretical physics. 

My research interest is in theoretical physics and machine learning (deep neural networks in particular).  


## News

* Dec 2019: Our paper [Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent
](https://arxiv.org/abs/1902.06720) is accepted at **NeurIPS 2019**. We will be presenting on [Thu Dec 12th 10:45 AM -- 12:45 PM @ East Exhibition Hall B + C #175.](https://nips.cc/Conferences/2019/Schedule?showEvent=13916)

* Oct 2019: Our new paper [On Empirical Comparisons of Optimizers for Deep Learning
](https://arxiv.org/abs/1910.05446) is out on ArXiv!  

* Jul 2019: [Measuring the Effects of Data Parallelism on Neural Network Training
](http://jmlr.org/papers/volume20/18-789/18-789.pdf) is now published at JMLR!

* Jul 2019: I started a new role as research scientist at Google Research, Brain Team.

* Jul 2019: Thanks everyone who contributed and participated at the [ICML 2019 Workshop on Theoretical Physics for Deep Learning](https://sites.google.com/view/icml2019phys4dl). Talks and slides are now [available](https://sites.google.com/view/icml2019phys4dl/schedule?authuser=0).

* Feb 2019: Proposal for [ICML 2019](https://icml.cc/) Workshop on Theoretical Physics for Deep Learning has been accepted! 

* Feb 2019: Our new paper [Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent
](https://arxiv.org/abs/1902.06720) is out on ArXiv!  

## Publication

[[Google Scholar]](https://scholar.google.com/citations?user=d3YhiooAAAAJ&hl=en) [[arXiv]](https://arxiv.org/a/lee_j_7.html)  

* **On Empirical Comparisons of Optimizers for Deep Learning**  
Dami Choi, Christopher J. Shallue, Zachary Nado, **Jaehoon Lee**, Chris J. Maddison, George E. Dahl  
[[arXiv: 1910.05446]](https://arxiv.org/abs/1910.05446)

* **Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent**  
**Jaehoon Lee**\*, Lechao Xiao\*, Samuel S. Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, Jeffrey Pennington  
[Neural Information Processing Systems (NeurIPS), 2019.](https://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent)  
[[arXiv: 1902.06720]](https://arxiv.org/abs/1902.06720) [[code1]](https://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/function_space_linearization.ipynb) [[code2]](https://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/weight_space_linearization.ipynb)


* **Measuring the Effects of Data Parallelism on Neural Network Training**  
Christopher J. Shallue\*, **Jaehoon Lee**\*, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, George E. Dahl  
[Journal of Machine Learning Research, 2019.](http://jmlr.org/papers/volume20/18-789/18-789.pdf)  
[[arXiv: 1811.03600]](https://arxiv.org/abs/1811.03600)

* **Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes**  
Roman Novak\*, Lechao Xiao\*, **Jaehoon Lee**&, Yasaman Bahri&, Greg Yang, Daniel A. Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein  
International Conference on Learning Representations (ICLR), 2019.  
[[arXiv: 1810.05148]](https://arxiv.org/abs/1810.05148)


* **Deep Neural Networks as Gaussian Processes**  
**Jaehoon Lee**\*, Yasaman Bahri\*, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein  
International Conference on Learning Representations (ICLR), 2018.  
[[arXiv: 1711.00165]](https://arxiv.org/abs/1711.00165) [[code]](https://github.com/brain-research/nngp)

* **3d N=2 minimal SCFTs from Wrapped M5-branes**  
Jin-Beom Bae\*, Dongmin Gang\*, **Jaehoon Lee**\*  
Journal of High Energy Physics (JHEP), 2017.  
[[arXiv: 1610.09259]](https://arxiv.org/abs/1610.09259)

* **Linking dynamical heterogeneity to static amorphous order**  
Patrick Charbonneau\*, Ethan Dyer\*, **Jaehoon Lee**\*, Sho Yaida\*  
Journal of Statistical Mechanics: Theory and Experiment, 2016.  
[[arXiv: 1309.5085]](https://arxiv.org/abs/1309.5085)

* **Entanglement entropy from one-point functions in holographic states**  
Matthew J. S. Beach\*, **Jaehoon Lee**\*, Charles Rabideau\*, Mark Van Raamsdonk\*  
Journal of High Energy Physics (JHEP), 2016.  
[[arXiv: 1604.05308]](https://arxiv.org/abs/1604.05308)

* **Studies of superconformal field theories using GLSM and conformal bootstrap**  
**Jaehoon Lee**  
PhD Thesis, Massachusetts Institute of Technology  
[[Thesis]](https://dspace.mit.edu/handle/1721.1/99308)

* **Glassy slowdown and replica-symmetry-breaking instantons**  
Allan Adams\*, Tarek Anous\*, **Jaehoon Lee**\*, Sho Yaida\*  
Physical Review E (PRE), 2015.  
[[arXiv: 1406.1498]](https://arxiv.org/abs/1406.1498)

* **Exact Correlators of BPS Operators from the 3d Superconformal Bootstrap**  
Shai M. Chester\*, **Jaehoon Lee**\*, Silviu S. Pufu\*, Ran Yacoby\*   
Journal of High Energy Physics (JHEP), 2015.  
[[arXiv: 1412.0334]](https://arxiv.org/abs/1412.0334)

* **The N=8 Superconformal Bootstrap in Three Dimensions**  
Shai M. Chester\*, **Jaehoon Lee**\*, Silviu S. Pufu\*, Ran Yacoby\*  
Journal of High Energy Physics (JHEP), 2014.  
[[arXiv: 1406.4814]](https://arxiv.org/abs/1406.4814)

* **Algebra of Majorana Doubling**  
**Jaehoon Lee**\*, Frank Wilczek\*  
Physics Review Letters (PRL), 2013.  
[[arXiv: 1307.3245]](https://arxiv.org/abs/1307.3245)

* **GLSMs for non-Kahler Geometries**  
Allan Adams\*, Ethan Dyer\*, **Jaehoon Lee**\*  
Journal of High Energy Physics (JHEP), 2013.  
[[arXiv: 1206.5815]](https://arxiv.org/abs/1206.5815)

## Research

* Recent research interests include:
  1. Interplay between physics and machine learning
  2. Theoretical aspects of deep neural networks
  3. Scientific and principled study of deep neural networks and their learning algorithms
  4. Theoretical physics with focus on high energy physics
  

* Services:
  1. Reviewer for ICLR / ICML / NeurIPS
  2. Organizer for [Aspen Winter Conference on Physics for Machine Learning](https://sites.google.com/corp/view/phys4ml/)
  3. Organizer for ICML Workshop on Theoretical Physics for Deep Learning
  4. Organizer for Vancouver deep learning study group
